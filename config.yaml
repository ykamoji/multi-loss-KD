Common:
  Results: Results/
  Metrics:
    Name: acc,topk
    CachePath: metrics/
  DataSet:
    Name: imageNet
    Label: label
    Path: !ENV ${DATASET_PATH} # Update the dataset path here or add DATASET_PATH to environment.
    Train: '' # Set <>% or <> exact number of batches to train
    Test: '' # Set <>% or <> exact number of batches to test

FineTuning:
  Action: False
  Model:
    Name: google/vit-large-patch16-224
    CachePath : model/
    OutputPath: tuned-model/
    Index: -1
    LoadCheckPoint: True
    CheckPointIndex: -1
  Hyperparameters:
    TrainBatchSize: 8
    EvalBatchSize: 32
    Epochs: 1
    Lr: 0.00005
    WeightDecay: 0.3
    WarmUpRatio: 0.03
    Steps:
      SaveSteps: 1000
      EvalSteps: 1000
      LoggingSteps: 100
      GradientAccumulation: 1

Distillation:
  Action: False
  Model:
    UseLocal: False
    Name: google/vit-large-patch16-224
    CachePath: model/
    OutputPath: tuned-model/
    Index: -1   # set -1 to perform distillation with the last fine-tuned model of that dataset otherwise set the index manually.
  StudentModel:
    Name: google/vit-base-patch16-224
    CachePath: model/
    OutputPath: distilled-model/
    Index: -1
    LoadCheckPoint: True
    CheckPointIndex: -1
  Hyperparameters:
    TrainBatchSize: 8
    EvalBatchSize: 32
    Epochs: 1
    Lr: 0.00005
    WeightDecay: 0.3
    WarmUpRatio: 0.03
    Steps:
      SaveSteps: 1000
      EvalSteps: 1000
      LoggingSteps: 100
      GradientAccumulation: 1
    KD:
      Alpha: 0.5
      Temperature: 5
  RandomWeights: False
  UseDistTokens: False
  DistillationType: soft # Set soft for KL divergence loss using probability or hard for cross entropy loss with logits
  UseAttributionLoss: False
  UseATSLoss : False
  UseAttentionLoss: False
  UseHiddenLoss : False